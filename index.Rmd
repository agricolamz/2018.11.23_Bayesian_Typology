---
title: "Возможен ли отказ от выборки? Байесовский подход к типологическому исследованию"
author: "[Г. Мороз](mailto:agricolamz@gmail.com) (Лаборатория языковой конвергенции)"
date: "XV конференция по типологии и грамматике для молодых исследователей<br> Санкт-Петербург, 23.11.2018"
bibliography: bibliography.bib
output: slidy_presentation
font-import: https://fonts.googleapis.com/css?family=Fanwood+Text
font-family: 'Fanwood Text'
---

## Два вида статистики

В статистических исследованиях существует несколько школ, основые (см. [@efron16]):

1) фриквентистская
2) байесовская

## Разница подходов

В любой задаче мы пытаемся оценить некоторый параметр θ генеральной совокупности.

### Фриквентисткий подход

Фриквентисткий подход исходит из идеи, что есть некоторая генеральная совокупность из которой мы берем выборки и оцениваем параметр θ̂. Таким образом параметр считается фиксированным, а оценка, полученная на основе выборки --- результат независимого эксперимента. Результаты разных экспериментов должны быть основаны на разных выборках, а различия, полученные в результате разных экспериментов объясняются флуктуацией выборки.

* параметр θ -- фиксирован
* данные варьируются

### Байесовский подход
В байесовском подходе всему, в том числе оцениваемому параметру приписывается вероятностное распределение, которое получается по формуле Байеса из априорного распределения (некоторые изначальные предположения о природе θ) и оценки, полученных из выборки.

* параметр θ -- носит вероятностный характер
* данные фиксированы

## Подходы к типологическому исследованию

### Фриквентистский

* Создаем **репрезентативную** выборку языков
* Оцениваем интересующий нас параметр
* При повторном исследовании пересобираем выборку

### Байесовский

* Собираем в априорное распределение наши ожидания, предыдущие работы и т. п.
* Создаем выборку языков
* Обновляем наши ожидания при помощи данных из выборки

## Что потом можно сделать с полученными оценками?

* написать статью
* построить марковские цепи [Manova 2000], [Widmann 2001]

Переходы из одного типа в другой:

| P(VSO → VSO) | P(SVO → VSO) | P(SOV → VSO) |
|--------------|--------------|--------------|
| P(VSO → SVO) | P(SVO → SVO) | P(SOV → SVO) |
| P(VSO → SOV) | P(SVO → SOV) | P(SOV → SOV) |

Произвольные вероятности из [Widmann 2001]:

| 0.90 | 0.02 | 0.01 |
|------|------|------|
| 0.09 | 0.90 | 0.09 |
| 0.01 | 0.08 | 0.90 |

## Markov chains [Widmann 2001]

Type 1: VSO; Type 2: SVO; Type 3: SOV

![](manova.png)


## Недостатки фриквентистского подхода: репрезентативность выборки

* [@bell78] "Language Samples"
* [@dryer89] "Large Linguistic Areas and Language Sampling"
* [@perkins89] "Statistical Techniques for Determining Language Sample Size"
* [@nichols92] "Linguistic Diversity in Space and Time"
* [@rietveld93] "Statistical Techniques for the Study of Language and Language Behaviour"
* [@rijkhoff98] "Language sampling"
* [@maslova00] "A dynamic approach to the verification of distributional universals"
* [@widmann01] "Language Sampling for Typological Studies"
* [@janssen06] "Randomization Tests in Language Typology"
* [@bakker10] "Language Sampling"

## Недостатки фриквентистского подхода: репрезентативность выборки

* генетические
* контакты
* ? культурные
* типологические
* библиографические
* популяционные

## Недостатки байесовского подхода: откуда брать априорное распределение

* эта технику ругают за субъективизм
* математическая сложность (лингвистам трудно все, что не пропорция/проценты)

### Мое предложение

* Исследуем все доступные материалы по всем семьям
* Нет выборкам (но если очень нужно их можно делать из всех доступных данных)
* Не обязательно ждать конца исследования, чтобы получить результаты (Байесовское мышление)
* Сравниваем внутригрупповую дисперсию признака с дисперсией по всем группам
  * получаем ответ на вопрос, что редко, что часто
  * получаем "ответ" на вопрос: типологическое  vs. ареальное vs. генеалогическое
* Empirical Bayes and Missing species problem

## Спасибо!

Г. Мороз (agricolamz@gmail.com)

Ссылка на презентацию: 

## Referenses {.smaller}
